{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing much of polar_pomdp0.45 for problem structure, particle filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ParticleFilters\n",
    "using Distributions\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using StatsBase\n",
    "#using Reel\n",
    "using SparseArrays\n",
    "using GridInterpolations\n",
    "using DataStructures\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"atan2.jl\")\n",
    "include(\"obs_rel.jl\")\n",
    "include(\"polargrid_rel_qual.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(2)\n",
    "TGT_SPD = 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly determine next target course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function next_crs(crs,rng)\n",
    "    if rand(rng) < .9\n",
    "        return crs\n",
    "    end\n",
    "    crs = (crs + rand(rng,[-1,1])*30) % 360\n",
    "    if crs < 0 crs += 360 end\n",
    "    return crs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state as tuple (x, y, crs, spd) of target (spd of o/s)\n",
    "function f(state, control, rng)\n",
    "    r, θ, crs, spd = state\n",
    "    θ += control[1]\n",
    "    spd = control[2]\n",
    "    if θ < 0 θ += 360 end\n",
    "    θ = θ % 360\n",
    "    crs -= control[1]\n",
    "    if crs < 0 crs += 360 end\n",
    "    crs = crs % 360\n",
    "    x = r*cos(π/180*θ)\n",
    "    y = r*sin(π/180*θ)\n",
    "    pos = [x + TGT_SPD*cos(π/180*crs) - spd, y + \n",
    "        TGT_SPD*sin(π/180*crs)]\n",
    "    crs = next_crs(crs,rng)\n",
    "    r = sqrt(pos[1]^2 + pos[2]^2)\n",
    "    θ = atan2(pos[1],pos[2])*180/π\n",
    "    if θ < 0 θ += 360 end\n",
    "    return (r, θ, crs, spd)::NTuple{4, Real}\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for f that returns vector rather than Tuple for particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f2(x, u, rng)\n",
    "    temp = [i for i in f(x, u, rng)]\n",
    "    return temp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r(s)\n",
    "    range = s[1]\n",
    "    if range > 150 return -.1 end  # reward to not lose track of contact\n",
    "    if range <= 10 return -1 end  # collision avoidance\n",
    "    return .1  # being in \"sweet spot\" maximizes reward\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space and function to convert from action to index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ((-30,1), (-30, 2), (0, 1), (0, 2), (30, 1), (30, 2))\n",
    "\n",
    "action_to_index(a) = trunc(Int, 2*(a[1]/30+1) + a[2])\n",
    "\n",
    "function index_to_action(a)\n",
    "    if a % 2 == 0\n",
    "        return ( trunc(Int,(((a - 2) / 2) - 1) * 30), 2)\n",
    "    else\n",
    "        return ( trunc(Int,(((a - 1) / 2) - 1) * 30), 1)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used for our belief state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "model = ParticleFilterModel{Vector{Float64}}(f2, g)\n",
    "pfilter = SIRParticleFilter(model, num_particles)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index of optimal action using current Q values and possibly the exploration bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arg_max_action(history, exploration_bonus=false)\n",
    "    \n",
    "    # only need to compute if exploration possibility\n",
    "    if exploration_bonus\n",
    "        N_h = 0\n",
    "        for action in action_to_index.(action_space)\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            N_h += N[new_index]\n",
    "        end    \n",
    "    end\n",
    "    \n",
    "    values = Float64[]\n",
    "    for action in action_to_index.(action_space)\n",
    "        \n",
    "        new_index = copy(history)\n",
    "        append!(new_index, action)\n",
    "        \n",
    "        # best action with exploration possibility\n",
    "        if exploration_bonus\n",
    "            append!(values, Q[new_index] + c * sqrt(log(N_h) / N([new_index])))\n",
    "            \n",
    "        # strictly best action\n",
    "        else\n",
    "            append!(values, Q[new_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return argmax(values)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rollout with random actions until we reach satisfactory depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout_random(state, depth)\n",
    "    \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    # random action\n",
    "    random_action_index = rand(rng,action_to_index.(action_space))\n",
    "    action = index_to_action(random_action_index)\n",
    "    \n",
    "    # generate next state and reward with random action; observation doesn't matter\n",
    "    state_prime = f2(state, action, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    return reward + lambda * rollout_random(state_prime, depth-1)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate function includes search, expansion, and rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulate(state, history, depth)\n",
    "   \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    \n",
    "    # expansion\n",
    "    test_index = copy(history)\n",
    "    append!(test_index, 1)\n",
    "    \n",
    "    if !haskey(Q, test_index)\n",
    "        for action in action_to_index.(action_space)\n",
    " \n",
    "            # initialize Q and N to zeros\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            Q[new_index] = 0\n",
    "            N[new_index] = 0\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rollout_random(state, depth)\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # find optimal action to explore\n",
    "    search_action_index = arg_max_action(history, exploration_bonus=true)\n",
    "    action = index_to_action(search_action_index)\n",
    "    \n",
    "    # take action; get new state, observation, and reward\n",
    "    state_prime = f2(state, action, rng)\n",
    "    observation = h(state_prime, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    # recursive call after taking action and getting observation\n",
    "    new_history = copy(history)\n",
    "    append!(new_history, search_action_index)\n",
    "    append!(new_history, observation)\n",
    "    q = reward + lambda * simulate(state_prime, new_history, depth-1)\n",
    "    \n",
    "    # update counts and values\n",
    "    update_index = copy(history)\n",
    "    append!(update_index, search_action_index)\n",
    "    N[update_index] += 1\n",
    "    Q[update_index] += ((q - Q[update_index]) / N[update_index])\n",
    "    \n",
    "    return q\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main MCTS function; called by MCTS wrapper at each time step to choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function select_action(belief, depth)\n",
    "    \n",
    "    # empty history at top recursive call\n",
    "    history = Int64[]\n",
    "    \n",
    "    # loop\n",
    "    # counter for now, switch to time later?\n",
    "    counter = 0\n",
    "    while counter < 100 # probably increase; small for debugging\n",
    "        \n",
    "        # draw state randomly based on belief state (pick a random particle)\n",
    "        state = rand(rng,belief)\n",
    "        \n",
    "        # simulate\n",
    "        simulate(state, history, depth)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    best_action_index = arg_max_action(history)\n",
    "    action = index_to_action(best_action_index)\n",
    "    return action\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to advance history tree after an action is chosen and observation is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modify_history_tree(last_action, last_obs)\n",
    "    \n",
    "    newQ = Dict{Array{Int64,1},Float64}()\n",
    "    newN = Dict{Array{Int64,1},Float64}()\n",
    "    \n",
    "    for key in keys(Q)\n",
    "        if key[0] == last_action && key[1] == last_obs\n",
    "            newQ[key[3:length(key)]] = Q[key]\n",
    "            newN[key[3:length(key)]] = N[key]\n",
    "        else\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (newQ, newN)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true state\n",
    "# for now state is [range, bearing, relative course, own speed]\n",
    "# assume a starting position within range of sensor and not too close\n",
    "true_state = [rand(rng, 25:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "# belief state\n",
    "# assume perfect knowledge at first time step\n",
    "belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation prep/initialization; for now we start with no prior knowledge for Q values/N values, could incorporate this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "\n",
    "# global Q and N dictionaries, indexed by history (and optionally action to follow all in same array; using ints)\n",
    "Q = Dict{Array{Int64,1},Float64}()\n",
    "N = Dict{Array{Int64,1},Float64}()\n",
    "\n",
    "# lambda, discount factor\n",
    "lambda = 0.9\n",
    "\n",
    "# exploration factor, experiment with different values\n",
    "c = 1\n",
    "\n",
    "# experiment with different depth parameters \n",
    "depth = 5\n",
    "\n",
    "# 500 time steps with an action to be selected at each\n",
    "num_iters = 500\n",
    "\n",
    "action = nothing\n",
    "observation = nothing\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 time step simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step = 1:num_iters\n",
    "    \n",
    "    # if action taken, modify history tree\n",
    "    if action != nothing\n",
    "        (Q,N) = modify_history_tree(last_action, last_obs)\n",
    "    end\n",
    "    \n",
    "    # select an action\n",
    "    action = select_action(belief, depth)\n",
    "    \n",
    "    # take action; get next true state, obs, and reward\n",
    "    next_state = f2(true_state, action, rng)\n",
    "    observation = h(next_state, rng)\n",
    "    reward = r(Tuple(next_state))\n",
    "    true_state = next_state\n",
    "    \n",
    "    # update belief state (particle filter)\n",
    "    belief = update(pfilter, belief, action, observation)\n",
    "    \n",
    "    # accumulate reward\n",
    "    total_reward += reward\n",
    "    # might want to keep track of each step, could use an array to track states, reward, actions, obs\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
