{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing much of polar_pomdp0.45 for problem structure, particle filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ParticleFilters\n",
    "using Distributions\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using StatsBase\n",
    "#using Reel\n",
    "using SparseArrays\n",
    "using GridInterpolations\n",
    "using DataStructures\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"atan2.jl\")\n",
    "include(\"obs_rel.jl\")\n",
    "include(\"polargrid_rel_qual.jl\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(2)\n",
    "TGT_SPD = 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly determine next target course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function next_crs(crs,rng)\n",
    "    if rand(rng) < .9\n",
    "        return crs\n",
    "    end\n",
    "    crs = (crs + rand(rng,[-1,1])*30) % 360\n",
    "    if crs < 0 crs += 360 end\n",
    "    return crs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state as tuple (x, y, crs, spd) of target (spd of o/s)\n",
    "function f(state, control, rng)\n",
    "    r, θ, crs, spd = state\n",
    "    θ += control[1]\n",
    "    spd = control[2]\n",
    "    if θ < 0 θ += 360 end\n",
    "    θ = θ % 360\n",
    "    crs -= control[1]\n",
    "    if crs < 0 crs += 360 end\n",
    "    crs = crs % 360\n",
    "    x = r*cos(π/180*θ)\n",
    "    y = r*sin(π/180*θ)\n",
    "    pos = [x + TGT_SPD*cos(π/180*crs) - spd, y + \n",
    "        TGT_SPD*sin(π/180*crs)]\n",
    "    crs = next_crs(crs,rng)\n",
    "    r = sqrt(pos[1]^2 + pos[2]^2)\n",
    "    θ = atan2(pos[1],pos[2])*180/π\n",
    "    if θ < 0 θ += 360 end\n",
    "    return (r, θ, crs, spd)::NTuple{4, Real}\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for f that returns vector rather than Tuple for particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f2(x, u, rng)\n",
    "    temp = [i for i in f(x, u, rng)]\n",
    "    return temp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r(s)\n",
    "    range = s[1]\n",
    "    if range > 150 return -.1 end  # reward to not lose track of contact\n",
    "    if range <= 10 return -1 end  # collision avoidance\n",
    "    return .1  # being in \"sweet spot\" maximizes reward\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space and function to convert from action to index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ((-30,1), (-30, 2), (0, 1), (0, 2), (30, 1), (30, 2))\n",
    "\n",
    "action_to_index(a) = trunc(Int, 2*(a[1]/30+1) + a[2])\n",
    "\n",
    "function index_to_action(a)\n",
    "    if a % 2 == 0\n",
    "        return ( trunc(Int,(((a - 2) / 2) - 1) * 30), 2)\n",
    "    else\n",
    "        return ( trunc(Int,(((a - 1) / 2) - 1) * 30), 1)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used for our belief state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "model = ParticleFilterModel{Vector{Float64}}(f2, g)\n",
    "pfilter = SIRParticleFilter(model, num_particles)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index of optimal action using current Q values and possibly the exploration bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arg_max_action(history, c=nothing, exploration_bonus=false)\n",
    "    \n",
    "    # only need to compute if exploration possibility\n",
    "    if exploration_bonus\n",
    "        N_h = 0\n",
    "        for action in action_to_index.(action_space)\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            N_h += N[new_index]\n",
    "        end    \n",
    "    end\n",
    "    \n",
    "    values = Float64[]\n",
    "    for action in action_to_index.(action_space)\n",
    "        \n",
    "        new_index = copy(history)\n",
    "        append!(new_index, action)\n",
    "        \n",
    "        # best action with exploration possibility\n",
    "        if exploration_bonus\n",
    "            append!(values, Q[new_index] + c * sqrt(log(N_h) / N([new_index])))\n",
    "            \n",
    "        # strictly best action\n",
    "        else\n",
    "            append!(values, Q[new_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return argmax(values)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rollout with random actions until we reach satisfactory depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout_random(state, depth)\n",
    "    \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    # random action\n",
    "    random_action_index = rand(rng,action_to_index.(action_space))\n",
    "    action = index_to_action(random_action_index)\n",
    "    \n",
    "    # generate next state and reward with random action; observation doesn't matter\n",
    "    state_prime = f2(state, action, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    return reward + lambda * rollout_random(state_prime, depth-1)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate function includes search, expansion, and rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulate(state, history, depth, c)\n",
    "   \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    \n",
    "    # expansion\n",
    "    test_index = copy(history)\n",
    "    append!(test_index, 1)\n",
    "    \n",
    "    if !haskey(Q, test_index)\n",
    "        for action in action_to_index.(action_space)\n",
    " \n",
    "            # initialize Q and N to zeros\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            Q[new_index] = 0\n",
    "            N[new_index] = 0\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rollout_random(state, depth)\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # find optimal action to explore\n",
    "    search_action_index = arg_max_action(history, c=c, exploration_bonus=true)\n",
    "    action = index_to_action(search_action_index)\n",
    "    \n",
    "    # take action; get new state, observation, and reward\n",
    "    state_prime = f2(state, action, rng)\n",
    "    observation = h(state_prime, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    # recursive call after taking action and getting observation\n",
    "    new_history = copy(history)\n",
    "    append!(new_history, search_action_index)\n",
    "    append!(new_history, observation)\n",
    "    q = reward + lambda * simulate(state_prime, new_history, depth-1)\n",
    "    \n",
    "    # update counts and values\n",
    "    update_index = copy(history)\n",
    "    append!(update_index, search_action_index)\n",
    "    N[update_index] += 1\n",
    "    Q[update_index] += ((q - Q[update_index]) / N[update_index])\n",
    "    \n",
    "    return q\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main MCTS function; called by MCTS wrapper at each time step to choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function select_action(belief, depth, c)\n",
    "    \n",
    "    # empty history at top recursive call\n",
    "    history = Int64[]\n",
    "    \n",
    "    # loop\n",
    "    # timed loop, how long should intervals be?\n",
    "    #start_time = time_ns()\n",
    "    #while (time_ns() - start_time) / 1.0e9 < 1 # 1 second timer to start\n",
    "    \n",
    "    # counter for now, switch to time later\n",
    "    counter = 0\n",
    "    while counter < 100 # probably increase; small for debugging\n",
    "        \n",
    "        # draw state randomly based on belief state (pick a random particle)\n",
    "        state = rand(rng,belief)\n",
    "        \n",
    "        # simulate\n",
    "        simulate(state, history, depth, c)\n",
    "        \n",
    "        counter+=1\n",
    "    end\n",
    "    \n",
    "    best_action_index = arg_max_action(history)\n",
    "    action = index_to_action(best_action_index)\n",
    "    return action\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to advance history tree after an action is chosen and observation is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modify_history_tree(last_action, last_obs)\n",
    "    \n",
    "    newQ = Dict{Array{Int64,1},Float64}()\n",
    "    newN = Dict{Array{Int64,1},Float64}()\n",
    "    \n",
    "    for key in keys(Q)\n",
    "        if key[1] == last_action && key[2] == last_obs\n",
    "            newQ[key[3:length(key)]] = Q[key]\n",
    "            newN[key[3:length(key)]] = N[key]\n",
    "        else\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (newQ, newN)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 500 time step trial with MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trial(depth, c)\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    #belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    # assume we just know it's between 100-150 distance\n",
    "    belief = ParticleCollection([[rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1] for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Simulation prep/initialization; for now we start with no prior knowledge for Q values/N values, could incorporate this later\n",
    "    \n",
    "    # global Q and N dictionaries, indexed by history (and optionally action to follow all in same array; using ints)\n",
    "    Q = Dict{Array{Int64,1},Float64}()\n",
    "    N = Dict{Array{Int64,1},Float64}()\n",
    "\n",
    "    # global scope, not manipulating these parameters for now\n",
    "    # lambda, discount factor\n",
    "    #lambda = 0.9\n",
    "\n",
    "    # experiment with different depth parameters \n",
    "    depth = depth\n",
    "    # exploration factor, experiment with different values\n",
    "    c = c\n",
    "    \n",
    "    action = nothing\n",
    "    observation = nothing\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "       \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        # if action taken, modify history tree\n",
    "        if action != nothing\n",
    "            (Q,N) = modify_history_tree(action, observation)\n",
    "        end\n",
    "    \n",
    "    \n",
    "        # select an action\n",
    "        action = select_action(belief, depth, c)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = r(Tuple(next_state))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # update belief state (particle filter)\n",
    "        belief = update(pfilter, belief, action, observation)\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += reward\n",
    "        # might want to keep track of each step, could use an array to track states, reward, actions, obs\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 time step random action simulation, for comparison to MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function random_trial()\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "    \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        action = rand(rng, action_space)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = r(Tuple(next_state))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare random reward to MCTS of various depths\n",
    "\n",
    "Runs to find best c value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 runs complete\n",
      "20 runs complete\n",
      "30 runs complete\n",
      "40 runs complete\n",
      "50 runs complete\n",
      "60 runs complete\n",
      "70 runs complete\n",
      "80 runs complete\n",
      "90 runs complete\n",
      "100 runs complete\n",
      "\n",
      "Random reward\n",
      "[-37.80000000000027, -23.800000000000068, 50.00000000000044, -13.19999999999997, 4.6, 50.00000000000044, 11.999999999999973, -28.800000000000143, 50.00000000000044, 43.00000000000034, 46.000000000000384, -1.8000000000000014, -6.399999999999993, -7.999999999999991, -48.00000000000041, 12.199999999999973, -44.40000000000036, -44.20000000000036, 50.00000000000044, 49.40000000000043, -10.79999999999998, 39.20000000000029, 50.00000000000044, -45.400000000000375, -19.000000000000004, 42.40000000000039, 50.00000000000044, 7.599999999999989, 6.199999999999994, 50.00000000000044, 19.200000000000323, 50.00000000000044, -22.800000000000058, 50.00000000000044, 13.399999999999968, 48.00000000000041, -4.999999999999999, 12.99999999999997, 34.40000000000022, 14.799999999999963, 16.99999999999997, -1.8000000000000014, -49.20000000000043, 50.00000000000044, 31.00000000000017, -48.40000000000042, 43.80000000000035, -17.99999999999999, -15.599999999999962, -48.60000000000042, 26.600000000000108, -39.800000000000296, -42.20000000000033, -15.199999999999964, -5.399999999999999, 40.0000000000003, -37.600000000000264, -33.60000000000021, -43.80000000000035, -43.60000000000035, 37.600000000000264, 40.50000000000035, 10.999999999999977, -34.00000000000021, -15.399999999999963, 37.40000000000026, -28.200000000000134, 50.00000000000044, -35.40000000000023, 5.799999999999995, -45.00000000000037, -48.40000000000042, -16.799999999999972, 3.200000000000001, 14.999999999999963, 24.000000000000075, -27.80000000000013, 41.00000000000031, 40.60000000000031, -4.200000000000002, 48.90000000000044, 42.600000000000335, 32.60000000000019, 8.599999999999985, 21.400000000000034, -37.40000000000026, 8.799999999999985, 5.599999999999996, 47.600000000000406, 3.000000000000001, 30.400000000000162, 50.00000000000044, -25.00000000000009, 42.40000000000033, 43.200000000000344, -33.0000000000002, 3.8000000000000016, -33.60000000000021, 50.00000000000044, -16.799999999999972]\n",
      "Average: 6.05600000000006\n",
      "\n",
      "MCTS depth 1 c 5 reward\n",
      "[50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 33.60000000000021, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -11.599999999999975, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -17.999999999999986, -50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 32.40000000000019, 50.00000000000044, 50.00000000000044, 50.00000000000044, 11.599999999999975, -45.00000000000037, 7.999999999999988, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -37.20000000000026, 15.399999999999961, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 19.80000000000001, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 40.80000000000031, 50.00000000000044, 0.19999999999999937, 50.00000000000044, 40.2000000000003, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -2.2000000000000015, 45.60000000000038]\n",
      "Average: 42.836000000000375\n",
      "\n",
      "MCTS depth 5 c 5 reward\n",
      "[50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 3.4000000000000012, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 49.600000000000435, 17.799999999999983, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 42.20000000000033, 39.000000000000284, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 49.600000000000435, 50.00000000000044, 18.199999999999992, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 32.800000000000196, 50.00000000000044, 50.00000000000044, 48.200000000000415, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 23.000000000000057, 29.000000000000142, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044]\n",
      "Average: 48.02800000000041\n",
      "\n",
      "MCTS depth 10 c 5 reward\n",
      "[50.00000000000044, 49.80000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -20.40000000000002, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 46.800000000000395, 50.00000000000044, 50.00000000000044, 47.600000000000406, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 21.600000000000037, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 19.600000000000012, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 2.0, 49.80000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -10.399999999999979, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, 50.00000000000044, -44.40000000000036, 50.00000000000044]\n",
      "Average: 46.62000000000041\n"
     ]
    }
   ],
   "source": [
    "# global scope Q and N\n",
    "Q = Dict{Array{Int64,1},Float64}()\n",
    "N = Dict{Array{Int64,1},Float64}()\n",
    "# global scope params (that are not being experimented with)\n",
    "lambda = 0.95\n",
    "\n",
    "# collect total reward for each trial\n",
    "random_reward = Float64[]\n",
    "mcts_depth1_c5_reward = Float64[]\n",
    "mcts_depth5_c5_reward = Float64[]\n",
    "mcts_depth10_c5_reward = Float64[]\n",
    "\n",
    "# trials\n",
    "for i = 1:100\n",
    "    append!(random_reward, random_trial())\n",
    "    \n",
    "    append!(mcts_depth1_c5_reward, trial(1, 5))\n",
    "    append!(mcts_depth5_c5_reward, trial(5, 5))\n",
    "    append!(mcts_depth10_c5_reward, trial(10, 5))\n",
    "    \n",
    "    if i % 10 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "println(\"\")\n",
    "println(\"Random reward\")\n",
    "println(random_reward)\n",
    "println(\"Average: \", mean(random_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 1 c 5 reward\")\n",
    "println(mcts_depth1_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth1_c5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 5 c 5 reward\")\n",
    "println(mcts_depth5_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth5_c5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 10 c 5 reward\")\n",
    "println(mcts_depth10_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth10_c5_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
