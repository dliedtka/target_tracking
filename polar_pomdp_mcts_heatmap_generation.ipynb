{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS POMDP Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing much of polar_pomdp0.45 for problem structure, particle filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ParticleFilters\n",
    "using Distributions\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using StatsBase\n",
    "#using Reel\n",
    "using SparseArrays\n",
    "using GridInterpolations\n",
    "using DataStructures\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"atan2.jl\")\n",
    "include(\"obs_rel.jl\") # includes functions for generating observations for problem POMDP\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(2)\n",
    "TGT_SPD = 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly determine next target course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function next_crs(crs,rng)\n",
    "    if rand(rng) < .9\n",
    "        return crs\n",
    "    end\n",
    "    crs = (crs + rand(rng,[-1,1])*30) % 360\n",
    "    if crs < 0 crs += 360 end\n",
    "    return crs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state as tuple (x, y, crs, spd) of target (spd of o/s)\n",
    "function f(state, control, rng)\n",
    "    r, θ, crs, spd = state\n",
    "    θ += control[1]\n",
    "    spd = control[2]\n",
    "    if θ < 0 θ += 360 end\n",
    "    θ = θ % 360\n",
    "    crs -= control[1]\n",
    "    if crs < 0 crs += 360 end\n",
    "    crs = crs % 360\n",
    "    x = r*cos(π/180*θ)\n",
    "    y = r*sin(π/180*θ)\n",
    "    pos = [x + TGT_SPD*cos(π/180*crs) - spd, y + \n",
    "        TGT_SPD*sin(π/180*crs)]\n",
    "    crs = next_crs(crs,rng)\n",
    "    r = sqrt(pos[1]^2 + pos[2]^2)\n",
    "    θ = atan2(pos[1],pos[2])*180/π\n",
    "    if θ < 0 θ += 360 end\n",
    "    return (r, θ, crs, spd)::NTuple{4, Real}\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for f that returns vector rather than Tuple for particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f2(x, u, rng)\n",
    "    temp = [i for i in f(x, u, rng)]\n",
    "    return temp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r(s)\n",
    "    range = s[1]\n",
    "    if range > 150 return -.1  # reward to not lose track of contact\n",
    "        elseif range <= 10 return -1  # collision avoidance\n",
    "        elseif range <= 20 return 0.6\n",
    "        elseif range <= 30 return 0.7\n",
    "        elseif range <= 40 return 0.8\n",
    "        elseif range <= 50 return 0.9\n",
    "        elseif range <= 60 return 1\n",
    "        elseif range <= 70 return 0.9\n",
    "        elseif range <= 80 return 0.8\n",
    "        elseif range <= 90 return 0.7\n",
    "        elseif range <= 100 return 0.6\n",
    "        elseif range <= 110 return 0.5\n",
    "        elseif range <= 120 return 0.4\n",
    "        elseif range <= 130 return 0.3\n",
    "        elseif range <= 140 return 0.2\n",
    "        else return 0.1 end\n",
    "    return .1  # being in \"sweet spot\" maximizes reward\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space and function to convert from action to index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ((-30,1), (-30, 2), (0, 1), (0, 2), (30, 1), (30, 2))\n",
    "\n",
    "action_to_index(a) = trunc(Int, 2*(a[1]/30+1) + a[2])\n",
    "\n",
    "function index_to_action(a)\n",
    "    if a % 2 == 0\n",
    "        return ( trunc(Int,(((a - 2) / 2) - 1) * 30), 2)\n",
    "    else\n",
    "        return ( trunc(Int,(((a - 1) / 2) - 1) * 30), 1)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used for our belief state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "model = ParticleFilterModel{Vector{Float64}}(f2, g)\n",
    "pfilter = SIRParticleFilter(model, num_particles)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index of optimal action using current Q values and possibly the exploration bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arg_max_action(Q, N, history, c=nothing, exploration_bonus=false)\n",
    "    \n",
    "    # only need to compute if exploration possibility\n",
    "    if exploration_bonus\n",
    "        N_h = 0\n",
    "        for action in action_to_index.(action_space)\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            N_h += N[new_index]\n",
    "        end    \n",
    "    end\n",
    "    \n",
    "    values = Float64[]\n",
    "    for action in action_to_index.(action_space)\n",
    "        \n",
    "        new_index = copy(history)\n",
    "        append!(new_index, action)\n",
    "        \n",
    "        # best action with exploration possibility\n",
    "        if exploration_bonus\n",
    "            \n",
    "            # ensure an action chosen zero times is always chosen\n",
    "            if N[new_index] == 0\n",
    "                return action\n",
    "            end\n",
    "            \n",
    "            # compute exploration bonus, checking for zeroes (I don't think this will ever occur anyway...)\n",
    "            if log(N_h) < 0\n",
    "                numerator = 0\n",
    "            else\n",
    "                numerator = sqrt(log(N_h))\n",
    "            end\n",
    "            denominator = N[new_index]\n",
    "            exp_bonus = c * numerator / denominator\n",
    "            append!(values, Q[new_index] + exp_bonus)\n",
    "        \n",
    "        # strictly best action\n",
    "        else\n",
    "            append!(values, Q[new_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return argmax(values)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rollout with random actions until we reach satisfactory depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout_random(state, depth)\n",
    "    \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    # random action\n",
    "    random_action_index = rand(rng,action_to_index.(action_space))\n",
    "    action = index_to_action(random_action_index)\n",
    "    \n",
    "    # generate next state and reward with random action; observation doesn't matter\n",
    "    state_prime = f2(state, action, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    return reward + lambda * rollout_random(state_prime, depth-1)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate function includes search, expansion, and rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulate(Q, N, state, history, depth, c)\n",
    "    \n",
    "    if depth == 0 return (Q, N, 0) end\n",
    "    \n",
    "    \n",
    "    # expansion\n",
    "    test_index = copy(history)\n",
    "    append!(test_index, 1)\n",
    "    \n",
    "    if !haskey(Q, test_index)\n",
    "        \n",
    "        for action in action_to_index.(action_space)\n",
    "            # initialize Q and N to zeros\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            Q[new_index] = 0\n",
    "            N[new_index] = 0        \n",
    "        end\n",
    "\n",
    "        # rollout\n",
    "        return (Q, N, rollout_random(state, depth))\n",
    "        \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # find optimal action to explore\n",
    "    search_action_index = arg_max_action(Q, N, history, c, true)\n",
    "    action = index_to_action(search_action_index)\n",
    "    \n",
    "    # take action; get new state, observation, and reward\n",
    "    state_prime = f2(state, action, rng)\n",
    "    observation = h(state_prime, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    # recursive call after taking action and getting observation\n",
    "    new_history = copy(history)\n",
    "    append!(new_history, search_action_index)\n",
    "    append!(new_history, observation)\n",
    "    (Q, N, successor_reward) = simulate(Q, N, state_prime, new_history, depth-1, c)\n",
    "    q = reward + lambda * successor_reward\n",
    "    \n",
    "    # update counts and values\n",
    "    update_index = copy(history)\n",
    "    append!(update_index, search_action_index)\n",
    "    N[update_index] += 1\n",
    "    Q[update_index] += ((q - Q[update_index]) / N[update_index])\n",
    "    \n",
    "    return (Q, N, q)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main MCTS function; called by MCTS wrapper at each time step to choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function best_action_from_state(state, depth, c)\n",
    "    \n",
    "    Q = Dict{Array{Int64,1},Float64}()\n",
    "    N = Dict{Array{Int64,1},Float64}()\n",
    "    \n",
    "    # empty history at top recursive call\n",
    "    history = Int64[]\n",
    "    \n",
    "    # counted iterations for now, would switch to time for a production model\n",
    "    counter = 0\n",
    "    while counter < 100 # probably increase; small for debugging\n",
    "        \n",
    "        # simulate\n",
    "        simulate(Q, N, float(state), history, depth, c)\n",
    "        \n",
    "        counter+=1\n",
    "    end\n",
    "    \n",
    "    best_action_index = arg_max_action(Q, N, history)\n",
    "    return best_action_index # want index this time\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global scope params (that are not being experimented with)\n",
    "lambda = 0.95\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course = 0\n",
      "x = -200\n",
      "x = -190\n",
      "x = -180\n",
      "x = -170\n",
      "x = -160\n",
      "x = -150\n",
      "x = -140\n",
      "x = -130\n",
      "x = -120\n",
      "x = -110\n",
      "x = -100\n",
      "x = -90\n",
      "x = -80\n",
      "x = -70\n",
      "x = -60\n",
      "x = -50\n",
      "x = -40\n",
      "x = -30\n",
      "x = -20\n",
      "x = -10\n",
      "x = 0\n",
      "x = 10\n",
      "x = 20\n",
      "x = 30\n",
      "x = 40\n",
      "x = 50\n",
      "x = 60\n",
      "x = 70\n",
      "x = 80\n",
      "x = 90\n",
      "x = 100\n",
      "x = 110\n",
      "x = 120\n",
      "x = 130\n",
      "x = 140\n",
      "x = 150\n",
      "x = 160\n",
      "x = 170\n",
      "x = 180\n",
      "x = 190\n",
      "x = 200\n",
      "course = 30\n",
      "x = -200\n",
      "x = -190\n",
      "x = -180\n",
      "x = -170\n",
      "x = -160\n",
      "x = -150\n",
      "x = -140\n",
      "x = -130\n",
      "x = -120\n",
      "x = -110\n",
      "x = -100\n",
      "x = -90\n",
      "x = -80\n",
      "x = -70\n",
      "x = -60\n",
      "x = -50\n",
      "x = -40\n",
      "x = -30\n",
      "x = -20\n",
      "x = -10\n",
      "x = 0\n",
      "x = 10\n",
      "x = 20\n",
      "x = 30\n",
      "x = 40\n",
      "x = 50\n",
      "x = 60\n",
      "x = 70\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] simulate(::Dict{Array{Int64,1},Float64}, ::Dict{Array{Int64,1},Float64}, ::Array{Float64,1}, ::Array{Int64,1}, ::Int64, ::Int64) at ./In[12]:29",
      " [2] best_action_from_state(::Array{Float64,1}, ::Int64, ::Int64) at ./In[13]:14",
      " [3] top-level scope at ./In[15]:30"
     ]
    }
   ],
   "source": [
    "for course in [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "    @show course\n",
    "    \n",
    "    for speed in [1]\n",
    "        \n",
    "        best_actions = zeros(Int64, 401, 401)\n",
    "        \n",
    "        for x = -200:200\n",
    "            if x % 10 == 0\n",
    "                @show x\n",
    "            end\n",
    "    \n",
    "            for y = -200:200\n",
    "    \n",
    "                array_x = x + 201\n",
    "                array_y = y + 201\n",
    "        \n",
    "                # generate state\n",
    "                radius = sqrt(x ^ 2 + y ^ 2)\n",
    "                theta = atan2(x,y) * 180 / pi\n",
    "                if theta < 0\n",
    "                    theta += 360\n",
    "                elseif theta == 360\n",
    "                    theta = 0\n",
    "                end\n",
    "        \n",
    "                state = [radius, theta, course, speed]\n",
    "        \n",
    "                # select action\n",
    "                best_actions[array_x, array_y] = best_action_from_state(state, 5, 20)\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        \n",
    "        filename = string(\"mcts_heatmap/modifiedreward_depth5_course\", course, \"_speed1\", \".csv\")\n",
    "        CSV.write(filename,  DataFrame(best_actions), writeheader=false)\n",
    "        \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
