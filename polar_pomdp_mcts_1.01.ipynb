{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing much of polar_pomdp0.45 for problem structure, particle filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\T450S\\.julia\\compiled\\v1.2\\Plots\\ld3vC.ji for Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file C:\\Users\\T450S\\.julia\\compiled\\v1.2\\ParticleFilters\\vVum1.ji for ParticleFilters [c8b314e2-9260-5cf8-ae76-3be7461ca6d0]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file C:\\Users\\T450S\\.julia\\compiled\\v1.2\\GridInterpolations\\7sRrY.ji for GridInterpolations [bb4c363b-b914-514b-8517-4eb369bc008a]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file C:\\Users\\T450S\\.julia\\compiled\\v1.2\\DataFrames\\AR9oZ.ji for DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file C:\\Users\\T450S\\.julia\\compiled\\v1.2\\CSV\\HHBkp.ji for CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "using ParticleFilters\n",
    "using Distributions\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using StatsBase\n",
    "#using Reel\n",
    "using SparseArrays\n",
    "using GridInterpolations\n",
    "using DataStructures\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"atan2.jl\")\n",
    "include(\"obsv2.jl\") # includes functions for generating observations for problem POMDP\n",
    "include(\"rules.jl\") # rules-based action\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(2)\n",
    "TGT_SPD = 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly determine next target course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function next_crs(crs,rng)\n",
    "    if rand(rng) < .9\n",
    "        return crs\n",
    "    end\n",
    "    crs = (crs + rand(rng,[-1,1])*30) % 360\n",
    "    if crs < 0 crs += 360 end\n",
    "    return crs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state as tuple (x, y, crs, spd) of target (spd of o/s)\n",
    "function f(state, control, rng)\n",
    "    r, θ, crs, spd = state\n",
    "    θ += control[1]\n",
    "    spd = control[2]\n",
    "    if θ < 0 θ += 360 end\n",
    "    θ = θ % 360\n",
    "    crs -= control[1]\n",
    "    if crs < 0 crs += 360 end\n",
    "    crs = crs % 360\n",
    "    x = r*cos(π/180*θ)\n",
    "    y = r*sin(π/180*θ)\n",
    "    pos = [x + TGT_SPD*cos(π/180*crs) - spd, y + \n",
    "        TGT_SPD*sin(π/180*crs)]\n",
    "    crs = next_crs(crs,rng)\n",
    "    r = sqrt(pos[1]^2 + pos[2]^2)\n",
    "    θ = atan2(pos[1],pos[2])*180/π\n",
    "    if θ < 0 θ += 360 end\n",
    "    return (r, θ, crs, spd)::NTuple{4, Real}\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for f that returns vector rather than Tuple for particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f2(x, u, rng)\n",
    "    temp = [i for i in f(x, u, rng)]\n",
    "    return temp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r(s)\n",
    "    range = s[1]\n",
    "    if range > 250 return [0, -.1, 0] end  # reward to not lose track of contact\n",
    "    if range <= 10 return [-.1, 0, 0] end  # collision avoidance\n",
    "    return [0, 0, .1]  # being in \"sweet spot\" maximizes reward\n",
    "end\n",
    "\n",
    "function abs_r(reward::Array{Float64,1})\n",
    "    return reward[1] + reward[2] + reward[3]\n",
    "end\n",
    "\n",
    "function event_r(reward::Array{Float64,1})\n",
    "    return [1*(reward[1] < 0), 1*(reward[2] < 0)]\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space and function to convert from action to index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ((-30,1), (-30, 2), (0, 1), (0, 2), (30, 1), (30, 2))\n",
    "\n",
    "action_to_index(a) = trunc(Int, 2*(a[1]/30+1) + a[2])\n",
    "\n",
    "function index_to_action(a)\n",
    "    if a % 2 == 0\n",
    "        return ( trunc(Int,(((a - 2) / 2) - 1) * 30), 2)\n",
    "    else\n",
    "        return ( trunc(Int,(((a - 1) / 2) - 1) * 30), 1)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used for our belief state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "model = ParticleFilterModel{Vector{Float64}}(f2, g)\n",
    "pfilter = SIRParticleFilter(model, num_particles)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index of optimal action using current Q values and possibly the exploration bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arg_max_action(Q, N, history, c=nothing, exploration_bonus=false)\n",
    "    \n",
    "    # only need to compute if exploration possibility\n",
    "    if exploration_bonus\n",
    "        N_h = 0\n",
    "        for action in action_to_index.(action_space)\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            N_h += N[new_index]\n",
    "        end    \n",
    "    end\n",
    "    \n",
    "    values = Float64[]\n",
    "    for action in action_to_index.(action_space)\n",
    "        \n",
    "        new_index = copy(history)\n",
    "        append!(new_index, action)\n",
    "        \n",
    "        # best action with exploration possibility\n",
    "        if exploration_bonus\n",
    "            \n",
    "            # ensure an action chosen zero times is always chosen\n",
    "            if N[new_index] == 0\n",
    "                return action\n",
    "            end\n",
    "            \n",
    "            # compute exploration bonus, checking for zeroes (I don't think this will ever occur anyway...)\n",
    "            if log(N_h) < 0\n",
    "                numerator = 0\n",
    "            else\n",
    "                numerator = sqrt(log(N_h))\n",
    "            end\n",
    "            denominator = N[new_index]\n",
    "            exp_bonus = c * numerator / denominator\n",
    "            append!(values, Q[new_index] + exp_bonus)\n",
    "        \n",
    "        # strictly best action\n",
    "        else\n",
    "            append!(values, Q[new_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return argmax(values)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rollout with random actions until we reach satisfactory depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout_random(state, depth)\n",
    "    \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    # random action\n",
    "    random_action_index = rand(rng,action_to_index.(action_space))\n",
    "    action = index_to_action(random_action_index)\n",
    "    \n",
    "    # generate next state and reward with random action; observation doesn't matter\n",
    "    state_prime = f2(state, action, rng)\n",
    "    reward = abs_r(r(Tuple(state_prime)))\n",
    "    \n",
    "    return reward + lambda * rollout_random(state_prime, depth-1)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate function includes search, expansion, and rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulate(Q, N, state, history, depth, c)\n",
    "    \n",
    "    if depth == 0 return (Q, N, 0) end\n",
    "    \n",
    "    \n",
    "    # expansion\n",
    "    test_index = copy(history)\n",
    "    append!(test_index, 1)\n",
    "    \n",
    "    if !haskey(Q, test_index)\n",
    "        \n",
    "        for action in action_to_index.(action_space)\n",
    "            # initialize Q and N to zeros\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            Q[new_index] = 0\n",
    "            N[new_index] = 0        \n",
    "        end\n",
    "\n",
    "        # rollout\n",
    "        return (Q, N, rollout_random(state, depth))\n",
    "        \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # find optimal action to explore\n",
    "    search_action_index = arg_max_action(Q, N, history, c, true)\n",
    "    action = index_to_action(search_action_index)\n",
    "    \n",
    "    # take action; get new state, observation, and reward\n",
    "    state_prime = f2(state, action, rng)\n",
    "    observation = h(state_prime, rng)\n",
    "    reward = abs_r(r(Tuple(state_prime)))\n",
    "    \n",
    "    # recursive call after taking action and getting observation\n",
    "    new_history = copy(history)\n",
    "    append!(new_history, search_action_index)\n",
    "    append!(new_history, observation)\n",
    "    (Q, N, successor_reward) = simulate(Q, N, state_prime, new_history, depth-1, c)\n",
    "    q = reward + lambda * successor_reward\n",
    "    \n",
    "    # update counts and values\n",
    "    update_index = copy(history)\n",
    "    append!(update_index, search_action_index)\n",
    "    N[update_index] += 1\n",
    "    Q[update_index] += ((q - Q[update_index]) / N[update_index])\n",
    "    \n",
    "    return (Q, N, q)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main MCTS function; called by MCTS wrapper at each time step to choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function select_action(Q, N, belief, depth, c)\n",
    "    \n",
    "    # empty history at top recursive call\n",
    "    history = Int64[]\n",
    "    \n",
    "    # loop\n",
    "    # timed loop, how long should intervals be?\n",
    "    #counter = 0\n",
    "    #start_time = time_ns()\n",
    "    #while (time_ns() - start_time) / 1.0e9 < 1 # 1 second timer to start\n",
    "    \n",
    "    # counted iterations for now, would switch to time for a production model\n",
    "    counter = 0\n",
    "    while counter < 100 # probably increase; small for debugging\n",
    "        \n",
    "        # draw state randomly based on belief state (pick a random particle)\n",
    "        state = rand(rng,belief)\n",
    "        \n",
    "        # simulate\n",
    "        simulate(Q, N, float(state), history, depth, c)\n",
    "        \n",
    "        counter+=1\n",
    "    end\n",
    "    #println(counter, \" iterations\")\n",
    "    \n",
    "    best_action_index = arg_max_action(Q, N, history)\n",
    "    action = index_to_action(best_action_index)\n",
    "    return (Q, N, action)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to advance history tree after an action is chosen and observation is recorded\n",
    "\n",
    "NOTE: We ended up not using this function. With our problem structure, we found that restarting the MCTS algorithm at every time step yielded better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modify_history_tree(Q, N, last_action, last_obs)\n",
    "    \n",
    "    newQ = Dict{Array{Int64,1},Float64}()\n",
    "    newN = Dict{Array{Int64,1},Float64}()\n",
    "    \n",
    "    for key in keys(Q)\n",
    "        # if key matches last action and observation, becomes root in new tree\n",
    "        if length(key) > 2 && key[1] == action_to_index(last_action) && key[2] == last_obs\n",
    "            newQ[key[3:length(key)]] = Q[key]\n",
    "            newN[key[3:length(key)]] = N[key]\n",
    "        else\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (newQ, newN)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 500 time step trial with MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global scope params (that are not being experimented with)\n",
    "lambda = 0.95\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trial(depth, c)\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 25:100), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    #belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    # assume we just know it's between 25-100 distance\n",
    "    belief = ParticleCollection([[rand(rng, 25:100), rand(rng,0:359), rand(rng,0:11)*30, 1] for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Simulation prep/initialization; for now we start with no prior knowledge for Q values/N values, could incorporate this later\n",
    "    \n",
    "    # global Q and N dictionaries, indexed by history (and optionally action to follow all in same array; using ints)\n",
    "    Q = Dict{Array{Int64,1},Float64}()\n",
    "    N = Dict{Array{Int64,1},Float64}()\n",
    "\n",
    "    # not manipulating these parameters for now, in global scope\n",
    "    # lambda, discount factor\n",
    "    #lambda = 0.95\n",
    "\n",
    "    # experimenting with different parameter values\n",
    "    # experiment with different depth parameters \n",
    "    depth = depth\n",
    "    # exploration factor, experiment with different values\n",
    "    c = c\n",
    "    \n",
    "    # don't need to modify history tree at first time step\n",
    "    action = nothing\n",
    "    observation = nothing\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = [0, 0, 0]\n",
    "    \n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "       \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        # NOTE: we found restarting history tree at each time step yielded better results\n",
    "        # if action taken, modify history tree\n",
    "        if action != nothing\n",
    "            #(Q,N) = modify_history_tree(Q, N, action, observation)\n",
    "            Q = Dict{Array{Int64,1},Float64}()\n",
    "            N = Dict{Array{Int64,1},Float64}()\n",
    "        end\n",
    "    \n",
    "    \n",
    "        # select an action\n",
    "        (Q, N, action) = select_action(Q, N, belief, depth, c)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = abs_r(r(Tuple(next_state)))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # update belief state (particle filter)\n",
    "        belief = update(pfilter, belief, action, observation)\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += r(Tuple(next_state))\n",
    "        \n",
    "        # TODO: flags for collision, lost track, end of simulation lost track\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different depths (use c=20, 100 iterations per select action as pre-optimization defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside, but we switched the loop in the select action function to a timed loop, and doing so we saw that we were able to perform roughly 60,000 iterations per second for depth 1, 9,000 for depth 5, 7,000 for depth 10, 5,500 for depth 25, 4,000 for depth 50, and 2,500 for depth 100.\n",
    "\n",
    "We would expect to see better results with more iterations, but there is a tradeoff with time. We will use 100 iterations for parameter optimization strictly for timing purposes, but ideally we would use more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 runs complete\n",
      "2 runs complete\n",
      "3 runs complete\n",
      "4 runs complete\n",
      "5 runs complete\n",
      "6 runs complete\n",
      "7 runs complete\n",
      "8 runs complete\n",
      "9 runs complete\n",
      "10 runs complete\n",
      "11 runs complete\n",
      "12 runs complete\n",
      "13 runs complete\n",
      "14 runs complete\n",
      "15 runs complete\n",
      "16 runs complete\n",
      "17 runs complete\n",
      "18 runs complete\n",
      "19 runs complete\n",
      "20 runs complete\n",
      "\n",
      "MCTS depth 10 reward\n",
      "[6, 0]\n",
      "Average: 3.0\n",
      "\n",
      "MCTS depth 25 reward\n",
      "[1, 0]\n",
      "Average: 0.5\n"
     ]
    }
   ],
   "source": [
    "# collect total reward for each trial\n",
    "#mcts_depth1_reward = Float64[]\n",
    "#mcts_depth5_reward = Float64[]\n",
    "mcts_depth10_reward = [0, 0]\n",
    "mcts_depth25_reward = [0, 0]\n",
    "#mcts_depth50_reward = Float64[]\n",
    "#mcts_depth100_reward = Float64[]\n",
    "\n",
    "# trials\n",
    "for i = 1:20\n",
    "    \n",
    "    #append!(mcts_depth1_reward, trial(1, 20))\n",
    "    #append!(mcts_depth5_reward, trial(5, 20))\n",
    "    mcts_depth10_reward += event_r(trial(10, 20))\n",
    "    mcts_depth25_reward += event_r(trial(25, 20))\n",
    "    #append!(mcts_depth50_reward, trial(50, 20))\n",
    "    #append!(mcts_depth100_reward, trial(100, 20))\n",
    "    \n",
    "    if i % 1 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "\n",
    "#println(\"\")\n",
    "#println(\"MCTS depth 1 reward\")\n",
    "#println(mcts_depth1_reward)\n",
    "#println(\"Average: \", mean(mcts_depth1_reward))\n",
    "\n",
    "\n",
    "#println(\"\")\n",
    "#println(\"MCTS depth 5 reward\")\n",
    "#println(mcts_depth5_reward)\n",
    "#println(\"Average: \", mean(mcts_depth5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 10 reward\")\n",
    "println(mcts_depth10_reward)\n",
    "println(\"Average: \", mean(mcts_depth10_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 25 reward\")\n",
    "println(mcts_depth25_reward)\n",
    "println(\"Average: \", mean(mcts_depth25_reward))\n",
    "\n",
    "#println(\"\")\n",
    "#println(\"MCTS depth 50 reward\")\n",
    "#println(mcts_depth50_reward)\n",
    "#println(\"Average: \", mean(mcts_depth50_reward))\n",
    "\n",
    "#println(\"\")\n",
    "#println(\"MCTS depth 100 reward\")\n",
    "#println(mcts_depth100_reward)\n",
    "#println(\"Average: \", mean(mcts_depth100_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the depth doesn't really matter as we had only one non-max reward.  Obviously then, we would favor a smaller depth as it will run through iterations faster.  We will make sure of our results by running a hundred trials for less time intensive depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect total reward for each trial\n",
    "mcts_depth1_reward = Float64[]\n",
    "mcts_depth5_reward = Float64[]\n",
    "mcts_depth10_reward = Float64[]\n",
    "\n",
    "# trials\n",
    "for i = 1:100\n",
    "    \n",
    "    append!(mcts_depth1_reward, trial(1, 20))\n",
    "    append!(mcts_depth5_reward, trial(5, 20))\n",
    "    append!(mcts_depth10_reward, trial(10, 20))\n",
    "    \n",
    "    if i % 10 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "println(\"\")\n",
    "println(\"MCTS depth 1 reward\")\n",
    "println(mcts_depth1_reward)\n",
    "println(\"Average: \", mean(mcts_depth1_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 5 reward\")\n",
    "println(mcts_depth5_reward)\n",
    "println(\"Average: \", mean(mcts_depth5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 10 reward\")\n",
    "println(mcts_depth10_reward)\n",
    "println(\"Average: \", mean(mcts_depth10_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit curious why depth 10 underperforms (perhaps it gets too complicated the further we extrapolate; maybe we should be discounting more).  However, 1 and 5 appear to work perfectly.  Just to be certain, we'll make sure we always get maximum reward (using 1, which is the fastest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect total reward for each trial\n",
    "mcts_depth1_reward = Float64[]\n",
    "\n",
    "# trials\n",
    "for i = 1:1000\n",
    "    \n",
    "    append!(mcts_depth1_reward, trial(1, 20))\n",
    "    \n",
    "    if i % 100 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "println(\"\")\n",
    "println(\"MCTS depth 1 reward\")\n",
    "println(mcts_depth1_reward)\n",
    "println(\"Average: \", mean(mcts_depth1_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over a thousand trials, we always achieve the max reward with a depth of 1.  We time a single trial to for real world application purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time_ns()\n",
    "println(trial(1, 20))\n",
    "println((time_ns() - start_time) / 1.0e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select 500 actions in 1.52378689 seconds, or just over 328 actions per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare MCTS performance to random action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function random_trial()\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = [0, 0, 0]\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "    \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        action = rand(rng, action_space)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = abs_r(r(Tuple(next_state)))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += r(Tuple(next_state))\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules-based trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rules_trial(risk)\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = [0, 0, 0]\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "    \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        action = index_to_action(rules_action(true_state, risk))\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = abs_r(r(Tuple(next_state)))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += r(Tuple(next_state))\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare random reward to MCTS of various depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rules_action (generic function with 2 methods)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 runs complete\n",
      "20 runs complete\n",
      "30 runs complete\n",
      "40 runs complete\n",
      "50 runs complete\n",
      "60 runs complete\n",
      "70 runs complete\n",
      "80 runs complete\n",
      "90 runs complete\n",
      "100 runs complete\n",
      "110 runs complete\n",
      "120 runs complete\n",
      "130 runs complete\n",
      "140 runs complete\n",
      "150 runs complete\n",
      "160 runs complete\n",
      "170 runs complete\n",
      "180 runs complete\n",
      "190 runs complete\n",
      "200 runs complete\n",
      "\n",
      "Result: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "include(\"rules.jl\") # rules-based action\n",
    "# collect total reward for each trial\n",
    "rules_reward = [0, 0]\n",
    "\n",
    "# trials\n",
    "for i = 1:200\n",
    "    \n",
    "    rules_reward += event_r(rules_trial(0.8))\n",
    "    \n",
    "    if i % 10 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "# [close, far, good]\n",
    "println(\"\")\n",
    "print(\"Result: \")\n",
    "println(rules_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules perform perfectly with perfect information. But we do not have perfect information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rules_trial2(risk)\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    #print(mean(belief))\n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = [0, 0, 0]\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "    \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        action = index_to_action(rules_action(mean(belief), risk))\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = abs_r(r(Tuple(next_state)))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += r(Tuple(next_state))\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 runs complete\n",
      "20 runs complete\n",
      "30 runs complete\n",
      "40 runs complete\n",
      "50 runs complete\n",
      "60 runs complete\n",
      "70 runs complete\n",
      "80 runs complete\n",
      "90 runs complete\n",
      "100 runs complete\n",
      "110 runs complete\n",
      "120 runs complete\n",
      "130 runs complete\n",
      "140 runs complete\n",
      "150 runs complete\n",
      "160 runs complete\n",
      "170 runs complete\n",
      "180 runs complete\n",
      "190 runs complete\n",
      "200 runs complete\n",
      "\n",
      "Result: [3, 43]\n"
     ]
    }
   ],
   "source": [
    "include(\"rules.jl\") # rules-based action\n",
    "# collect total reward for each trial\n",
    "rules_reward = [0, 0]\n",
    "\n",
    "# trials\n",
    "for i = 1:200\n",
    "    \n",
    "    rules_reward += event_r(rules_trial2(0.9))\n",
    "    \n",
    "    if i % 10 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "# [close, far, good]\n",
    "println(\"\")\n",
    "print(\"Result: \")\n",
    "println(rules_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no plots at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
