{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing much of polar_pomdp0.45 for problem structure, particle filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ParticleFilters\n",
    "using Distributions\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using StatsBase\n",
    "#using Reel\n",
    "using SparseArrays\n",
    "using GridInterpolations\n",
    "using DataStructures\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"atan2.jl\")\n",
    "include(\"obs_rel.jl\")\n",
    "include(\"polargrid_rel_qual.jl\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(2)\n",
    "TGT_SPD = 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly determine next target course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function next_crs(crs,rng)\n",
    "    if rand(rng) < .9\n",
    "        return crs\n",
    "    end\n",
    "    crs = (crs + rand(rng,[-1,1])*30) % 360\n",
    "    if crs < 0 crs += 360 end\n",
    "    return crs\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state as tuple (x, y, crs, spd) of target (spd of o/s)\n",
    "function f(state, control, rng)\n",
    "    r, θ, crs, spd = state\n",
    "    θ += control[1]\n",
    "    spd = control[2]\n",
    "    if θ < 0 θ += 360 end\n",
    "    θ = θ % 360\n",
    "    crs -= control[1]\n",
    "    if crs < 0 crs += 360 end\n",
    "    crs = crs % 360\n",
    "    x = r*cos(π/180*θ)\n",
    "    y = r*sin(π/180*θ)\n",
    "    pos = [x + TGT_SPD*cos(π/180*crs) - spd, y + \n",
    "        TGT_SPD*sin(π/180*crs)]\n",
    "    crs = next_crs(crs,rng)\n",
    "    r = sqrt(pos[1]^2 + pos[2]^2)\n",
    "    θ = atan2(pos[1],pos[2])*180/π\n",
    "    if θ < 0 θ += 360 end\n",
    "    return (r, θ, crs, spd)::NTuple{4, Real}\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for f that returns vector rather than Tuple for particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f2(x, u, rng)\n",
    "    temp = [i for i in f(x, u, rng)]\n",
    "    return temp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r(s)\n",
    "    range = s[1]\n",
    "    if range > 150 return -.1 end  # reward to not lose track of contact\n",
    "    if range <= 10 return -1 end  # collision avoidance\n",
    "    return .1  # being in \"sweet spot\" maximizes reward\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space and function to convert from action to index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ((-30,1), (-30, 2), (0, 1), (0, 2), (30, 1), (30, 2))\n",
    "\n",
    "action_to_index(a) = trunc(Int, 2*(a[1]/30+1) + a[2])\n",
    "\n",
    "function index_to_action(a)\n",
    "    if a % 2 == 0\n",
    "        return ( trunc(Int,(((a - 2) / 2) - 1) * 30), 2)\n",
    "    else\n",
    "        return ( trunc(Int,(((a - 1) / 2) - 1) * 30), 1)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used for our belief state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "model = ParticleFilterModel{Vector{Float64}}(f2, g)\n",
    "pfilter = SIRParticleFilter(model, num_particles)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index of optimal action using current Q values and possibly the exploration bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arg_max_action(Q, N, history, c=nothing, exploration_bonus=false)\n",
    "    \n",
    "    # only need to compute if exploration possibility\n",
    "    if exploration_bonus\n",
    "        N_h = 0\n",
    "        for action in action_to_index.(action_space)\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            N_h += N[new_index]\n",
    "        end    \n",
    "    end\n",
    "    \n",
    "    values = Float64[]\n",
    "    for action in action_to_index.(action_space)\n",
    "        \n",
    "        new_index = copy(history)\n",
    "        append!(new_index, action)\n",
    "        \n",
    "        # best action with exploration possibility\n",
    "        if exploration_bonus\n",
    "            \n",
    "            # ensure an action chosen zero times is always chosen\n",
    "            if N[new_index] == 0\n",
    "                return action\n",
    "            end\n",
    "            \n",
    "            # compute exploration bonus, checking for zeroes (I don't think this will ever occur anyway...)\n",
    "            if log(N_h) < 0\n",
    "                numerator = 0\n",
    "            else\n",
    "                numerator = sqrt(log(N_h))\n",
    "            end\n",
    "            \n",
    "            denominator = N[new_index]\n",
    "            \n",
    "            exp_bonus = c * numerator / denominator\n",
    "            \n",
    "            append!(values, Q[new_index] + exp_bonus)\n",
    "        \n",
    "        # strictly best action\n",
    "        else\n",
    "            append!(values, Q[new_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return argmax(values)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rollout with random actions until we reach satisfactory depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout_random(state, depth)\n",
    "    \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    # random action\n",
    "    random_action_index = rand(rng,action_to_index.(action_space))\n",
    "    action = index_to_action(random_action_index)\n",
    "    \n",
    "    # generate next state and reward with random action; observation doesn't matter\n",
    "    state_prime = f2(state, action, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    return reward + lambda * rollout_random(state_prime, depth-1)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate function includes search, expansion, and rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulate(Q, N, state, history, depth, c)\n",
    "   \n",
    "    if depth == 0 return 0 end\n",
    "    \n",
    "    \n",
    "    # expansion\n",
    "    test_index = copy(history)\n",
    "    append!(test_index, 1)\n",
    "    \n",
    "    if !haskey(Q, test_index)\n",
    "        for action in action_to_index.(action_space)\n",
    " \n",
    "            # initialize Q and N to zeros\n",
    "            new_index = copy(history)\n",
    "            append!(new_index, action)\n",
    "            Q[new_index] = 0\n",
    "            N[new_index] = 0\n",
    "            \n",
    "        end\n",
    "        \n",
    "        return rollout_random(state, depth)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # find optimal action to explore\n",
    "    search_action_index = arg_max_action(Q, N, history, c, true)\n",
    "    action = index_to_action(search_action_index)\n",
    "    \n",
    "    # take action; get new state, observation, and reward\n",
    "    state_prime = f2(state, action, rng)\n",
    "    observation = h(state_prime, rng)\n",
    "    reward = r(Tuple(state_prime))\n",
    "    \n",
    "    # recursive call after taking action and getting observation\n",
    "    new_history = copy(history)\n",
    "    append!(new_history, search_action_index)\n",
    "    append!(new_history, observation)\n",
    "    q = reward + lambda * simulate(Q, N, state_prime, new_history, depth-1, c)\n",
    "    \n",
    "    # update counts and values\n",
    "    update_index = copy(history)\n",
    "    append!(update_index, search_action_index)\n",
    "    N[update_index] += 1\n",
    "    Q[update_index] += ((q - Q[update_index]) / N[update_index])\n",
    "    \n",
    "    return (Q, N, q)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main MCTS function; called by MCTS wrapper at each time step to choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function select_action(Q, N, belief, depth, c)\n",
    "    \n",
    "    # empty history at top recursive call\n",
    "    history = Int64[]\n",
    "    \n",
    "    # loop\n",
    "    # timed loop, how long should intervals be?\n",
    "    #start_time = time_ns()\n",
    "    #while (time_ns() - start_time) / 1.0e9 < 1 # 1 second timer to start\n",
    "    \n",
    "    # counter for now, switch to time later\n",
    "    counter = 0\n",
    "    while counter < 100 # probably increase; small for debugging\n",
    "        \n",
    "        # draw state randomly based on belief state (pick a random particle)\n",
    "        state = rand(rng,belief)\n",
    "        \n",
    "        # simulate\n",
    "        simulate(Q, N, float(state), history, depth, c) # HERE!!!\n",
    "        \n",
    "        counter+=1\n",
    "    end\n",
    "    \n",
    "    best_action_index = arg_max_action(Q, N, history)\n",
    "    action = index_to_action(best_action_index)\n",
    "    return (Q, N, action)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCTS loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to advance history tree after an action is chosen and observation is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modify_history_tree(last_action, last_obs)\n",
    "    \n",
    "    newQ = Dict{Array{Int64,1},Float64}()\n",
    "    newN = Dict{Array{Int64,1},Float64}()\n",
    "    \n",
    "    for key in keys(Q)\n",
    "        if length(key) > 2 && key[1] == action_to_index(last_action) && key[2] == last_obs\n",
    "            newQ[key[3:length(key)]] = Q[key]\n",
    "            newN[key[3:length(key)]] = N[key]\n",
    "        else\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (newQ, newN)\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 500 time step trial with MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trial(depth, c)\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    #belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    # assume we just know it's between 100-150 distance\n",
    "    belief = ParticleCollection([[rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1] for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Simulation prep/initialization; for now we start with no prior knowledge for Q values/N values, could incorporate this later\n",
    "    \n",
    "    # global Q and N dictionaries, indexed by history (and optionally action to follow all in same array; using ints)\n",
    "    Q = Dict{Array{Int64,1},Float64}()\n",
    "    N = Dict{Array{Int64,1},Float64}()\n",
    "\n",
    "    # global scope, not manipulating these parameters for now\n",
    "    # lambda, discount factor\n",
    "    #lambda = 0.95\n",
    "\n",
    "    # experiment with different depth parameters \n",
    "    depth = depth\n",
    "    # exploration factor, experiment with different values\n",
    "    c = c\n",
    "    \n",
    "    action = nothing\n",
    "    observation = nothing\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 5#00\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "       \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        # if action taken, modify history tree\n",
    "        if action != nothing\n",
    "            if true\n",
    "                println(\"\")\n",
    "                println(N)\n",
    "                println(\"\")\n",
    "                println(action_to_index(action))\n",
    "                println(observation)\n",
    "            end\n",
    "            (Q,N) = modify_history_tree(action, observation)\n",
    "            if true\n",
    "                println(\"\")\n",
    "                println(N)\n",
    "            end\n",
    "        end\n",
    "    \n",
    "    \n",
    "        # select an action\n",
    "        #action = select_action(belief, depth, c)\n",
    "        (Q, N, action) = select_action(Q, N, belief, depth, c)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = r(Tuple(next_state))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # update belief state (particle filter)\n",
    "        belief = update(pfilter, belief, action, observation)\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += reward\n",
    "        # might want to keep track of each step, could use an array to track states, reward, actions, obs\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 time step random action simulation, for comparison to MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "function random_trial()\n",
    "    \n",
    "    # Initialize true state and belief state (particle filter); we assume perfect knowledge at start of simulation (could experiment otherwise with random beliefs)\n",
    "    \n",
    "    # true state\n",
    "    # for now state is [range, bearing, relative course, own speed]\n",
    "    # assume a starting position within range of sensor and not too close\n",
    "    true_state = [rand(rng, 100:150), rand(rng,0:359), rand(rng,0:11)*30, 1]\n",
    "\n",
    "    # belief state\n",
    "    # assume perfect knowledge at first time step\n",
    "    belief = ParticleCollection([true_state for i in 1:num_particles])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # run simulation\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    # 500 time steps with an action to be selected at each\n",
    "    num_iters = 500\n",
    "    \n",
    "    for time_step = 1:num_iters\n",
    "    \n",
    "        #if time_step % 100 == 0 \n",
    "        #    @show time_step\n",
    "        #end\n",
    "    \n",
    "        action = rand(rng, action_space)\n",
    "    \n",
    "        # take action; get next true state, obs, and reward\n",
    "        next_state = f2(true_state, action, rng)\n",
    "        observation = h(next_state, rng)\n",
    "        reward = r(Tuple(next_state))\n",
    "        true_state = next_state\n",
    "    \n",
    "        # accumulate reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return total_reward\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dict([3, 2, 5] => 0.0,[1, 0, 3] => 2.0,[1, 1, 3] => 0.0,[1, 3, 1] => 1.0,[3, 0, 4] => 1.0,[5, 1, 3] => 0.0,[3, 1, 2] => 0.0,[6, 1, 5] => 0.0,[1, 2, 6] => 0.0,[2, 3, 4] => 0.0,[2, 3, 1] => 0.0,[4, 2, 5] => 0.0,[3, 2, 1] => 0.0,[4, 0, 4] => 2.0,[4, 1, 5] => 0.0,[3, 1, 4] => 0.0,[1, 3, 2] => 1.0,[2] => 17.0,[2, 0, 6] => 2.0,[3, 3, 5] => 0.0,[5, 1, 6] => 0.0,[6, 0, 5] => 2.0,[5, 3, 1] => 1.0,[6, 2, 6] => 0.0,[4, 3, 3] => 0.0,[3, 0, 3] => 2.0,[4] => 16.0,[3, 0, 6] => 1.0,[4, 0, 2] => 2.0,[6, 0, 3] => 2.0,[6, 0, 6] => 1.0,[6, 3, 3] => 0.0,[2, 2, 2] => 0.0,[4, 1, 4] => 0.0,[2, 2, 3] => 0.0,[4, 2, 1] => 0.0,[6, 1, 6] => 0.0,[1, 2, 4] => 0.0,[5, 1, 5] => 0.0,[6, 2, 1] => 0.0,[2, 2, 6] => 0.0,[3, 0, 2] => 2.0,[6, 2, 2] => 0.0,[6, 0, 4] => 2.0,[2, 3, 3] => 0.0,[1, 3, 4] => 0.0,[3, 1, 5] => 0.0,[1, 1, 4] => 0.0,[5, 0, 6] => 1.0,[5, 3, 2] => 0.0,[6, 3, 2] => 0.0,[4, 0, 1] => 2.0,[4, 3, 2] => 0.0,[3] => 17.0,[2, 2, 4] => 0.0,[3, 2, 2] => 0.0,[1, 1, 1] => 0.0,[3, 3, 2] => 1.0,[5, 1, 2] => 1.0,[6, 3, 5] => 0.0,[4, 3, 5] => 0.0,[2, 3, 2] => 0.0,[6, 0, 1] => 2.0,[4, 1, 2] => 0.0,[1, 0, 4] => 2.0,[1, 2, 1] => 1.0,[4, 2, 3] => 0.0,[2, 0, 1] => 3.0,[3, 3, 6] => 0.0,[5, 3, 3] => 0.0,[5, 2, 4] => 0.0,[1, 1, 6] => 0.0,[6, 1, 4] => 0.0,[2, 0, 3] => 2.0,[1, 3, 5] => 0.0,[4, 3, 6] => 0.0,[6, 3, 4] => 0.0,[6] => 16.0,[4, 2, 4] => 0.0,[3, 1, 6] => 0.0,[4, 0, 5] => 2.0,[2, 3, 6] => 0.0,[2, 0, 2] => 3.0,[5, 3, 5] => 0.0,[6, 2, 4] => 0.0,[5, 2, 3] => 0.0,[2, 2, 5] => 0.0,[2, 0, 5] => 2.0,[6, 1, 2] => 0.0,[4, 0, 6] => 2.0,[1, 0, 1] => 2.0,[1, 1, 2] => 0.0,[1, 1, 5] => 0.0,[6, 0, 2] => 2.0,[3, 3, 1] => 1.0,[1] => 17.0,[5, 0, 1] => 2.0,[1, 3, 3] => 0.0,[3, 2, 4] => 0.0,[6, 3, 1] => 1.0,[4, 3, 1] => 0.0,[3, 3, 3] => 1.0,[2, 3, 5] => 0.0,[3, 1, 1] => 0.0,[5, 0, 2] => 1.0,[6, 2, 3] => 0.0,[5, 2, 6] => 0.0,[1, 3, 6] => 0.0,[3, 3, 4] => 1.0,[1, 0, 5] => 1.0,[1, 2, 2] => 0.0,[4, 2, 2] => 0.0,[5, 2, 5] => 0.0,[6, 1, 3] => 0.0,[5, 0, 5] => 1.0,[4, 0, 3] => 2.0,[3, 2, 6] => 0.0,[1, 2, 3] => 0.0,[5, 0, 3] => 1.0,[2, 2, 1] => 0.0,[5, 1, 4] => 0.0,[4, 1, 3] => 0.0,[3, 0, 5] => 1.0,[2, 0, 4] => 2.0,[1, 0, 2] => 2.0,[5, 3, 6] => 0.0,[5, 0, 4] => 1.0,[3, 0, 1] => 2.0,[5, 2, 2] => 1.0,[5, 2, 1] => 1.0,[6, 2, 5] => 0.0,[5, 1, 1] => 1.0,[5, 3, 4] => 0.0,[1, 0, 6] => 1.0,[3, 2, 3] => 0.0,[5] => 16.0,[3, 1, 3] => 0.0,[1, 2, 5] => 0.0,[4, 3, 4] => 0.0,[4, 2, 6] => 0.0,[6, 3, 6] => 0.0,[6, 1, 1] => 0.0,[4, 1, 6] => 0.0,[4, 1, 1] => 0.0)\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Dict([6] => 1.0,[3] => 2.0,[4] => 2.0,[1] => 2.0,[5] => 1.0,[2] => 2.0)\n",
      "\n",
      "Dict([6] => 1.0,[3] => 2.0,[4] => 2.0,[1] => 2.0,[5] => 1.0,[2] => 2.0)\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "Dict([6] => 4.0,[3] => 4.0,[4] => 4.0,[1] => 4.0,[5] => 4.0,[2] => 4.0)\n",
      "\n",
      "Dict([6] => 4.0,[3] => 4.0,[4] => 4.0,[1] => 4.0,[5] => 4.0,[2] => 4.0)\n",
      "\n",
      "3\n",
      "0\n",
      "\n",
      "Dict([6] => 6.0,[3] => 7.0,[4] => 7.0,[1] => 7.0,[5] => 7.0,[2] => 6.0)\n",
      "\n",
      "Dict([6] => 6.0,[3] => 7.0,[4] => 7.0,[1] => 7.0,[5] => 7.0,[2] => 6.0)\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Dict([6] => 9.0,[3] => 9.0,[4] => 9.0,[1] => 10.0,[5] => 10.0,[2] => 10.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global scope params (that are not being experimented with)\n",
    "lambda = 0.95\n",
    "\n",
    "trial(2,20, Q, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare random reward to MCTS of various depths\n",
    "\n",
    "Runs to find best c value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global scope Q and N\n",
    "Q = Dict{Array{Int64,1},Float64}()\n",
    "N = Dict{Array{Int64,1},Float64}()\n",
    "# global scope params (that are not being experimented with)\n",
    "lambda = 0.95\n",
    "\n",
    "# collect total reward for each trial\n",
    "random_reward = Float64[]\n",
    "mcts_depth1_c5_reward = Float64[]\n",
    "mcts_depth5_c5_reward = Float64[]\n",
    "mcts_depth10_c5_reward = Float64[]\n",
    "\n",
    "# trials\n",
    "for i = 1:100\n",
    "    append!(random_reward, random_trial())\n",
    "    \n",
    "    append!(mcts_depth1_c5_reward, trial(1, 5))\n",
    "    append!(mcts_depth5_c5_reward, trial(5, 5))\n",
    "    append!(mcts_depth10_c5_reward, trial(10, 5))\n",
    "    \n",
    "    if i % 10 == 0\n",
    "        println(i, \" runs complete\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# print results\n",
    "println(\"\")\n",
    "println(\"Random reward\")\n",
    "println(random_reward)\n",
    "println(\"Average: \", mean(random_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 1 c 5 reward\")\n",
    "println(mcts_depth1_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth1_c5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 5 c 5 reward\")\n",
    "println(mcts_depth5_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth5_c5_reward))\n",
    "\n",
    "println(\"\")\n",
    "println(\"MCTS depth 10 c 5 reward\")\n",
    "println(mcts_depth10_c5_reward)\n",
    "println(\"Average: \", mean(mcts_depth10_c5_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
